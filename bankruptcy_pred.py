# -*- coding: utf-8 -*-
"""bankruptcy_pred (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1crCpOSoYszLDS-TsXfErY4KaHM6NUFfu
"""

import math
import numpy as np
import pandas as pd
import seaborn as sns
# from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
# from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTEENN
from sklearn.utils import resample
from matplotlib import pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
import joblib # For saving the scaler and the best model

# Read in the dataset into a pandas dataframe
df = pd.read_csv('american_bankruptcy.csv')

# Drop some columns from the dataset
df = df.drop(columns = ['company_name'])
df = df.drop(columns = ['year'])

# Display the basic structure of the dataset
display(df.head(10))

# Data Cleaning

# Find out how many missing values are in each column in the dataset
missing_values = df.isnull().sum()
print('Missing Values per Column:')
display(missing_values)

print('\n')

# Find out how many duplicate samples are in the dataset
duplicates = df.duplicated().sum()
print('Duplicates in the Dataset:', duplicates)

# Feature Engineering with Binary Encoding

# Display the class labels with their count in the dataset
print('Class Labels with Counts in the Dataset:')
display(df['status_label'].value_counts())

print('\n')

"""

# df = pd.get_dummies(df, columns=['status_label'])

# Feature Engineering with One-Hot Encoding
encoder = OneHotEncoder(sparse_output=False)

status_encoding = encoder.fit_transform(df[['status_label']])

df_status_encoding = pd.DataFrame(status_encoding, columns=encoder.get_feature_names_out(['status_label']))
# df_status_encoding.index = df.index

df = pd.concat([df, df_status_encoding], axis=1)

display(df)

"""

# Encoding 'alive' to 0 and 'failed' to 1
df['status_encoding'] = df['status_label'].map({'alive': 0, 'failed': 1}).astype(int)
# df['status_encoding'] = df['status_encoding'].astype(int)

df = df.drop(columns = ['status_label'])

print('Dataset After Binary Encoding \'alive\' to 0 and \'failed\' to 1:')
display(df.head(10))

print('\n')

# Display the encoded class labels with their count in the dataset
print('Encoded Class Labels with Counts in the Dataset:')
display(df['status_encoding'].value_counts())

# Square Normalization

df_square = df.copy()

columns_to_normalize = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6',
                       'X7', 'X8', 'X9', 'X10', 'X11', 'X12',
                       'X13', 'X14', 'X15', 'X16', 'X17', 'X18']

# Visualize the data before applying the transformation
df_square[columns_to_normalize].hist(bins=50, figsize=(12, 8), color='blue')
plt.suptitle('Before Square Normalization')
plt.tight_layout(pad=0.5)
plt.show()

# Apply the square transformation
for column in columns_to_normalize:
    df_square[column] = df_square[column].apply(lambda x: x**2)

# Visualize the data after the transformation
df_square[columns_to_normalize].hist(bins=50, figsize=(12, 8), color='red')
plt.suptitle('After Square Normalization')
plt.tight_layout(pad=0.5)
plt.show()

# Inverse Normalization

df_inverse = df.copy()

columns_to_normalize = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6',
                       'X7', 'X8', 'X9', 'X10', 'X11', 'X12',
                       'X13', 'X14', 'X15', 'X16', 'X17', 'X18']

# Visualize the data before the transformation
df_inverse[columns_to_normalize].hist(bins=50, figsize=(12, 8), color='blue')
plt.suptitle('Before Inverse Normalization')
plt.tight_layout(pad=0.5)
plt.show()

# Apply the inverse transformation
for column in columns_to_normalize:
    df_inverse[column] = df_inverse[column].apply(lambda x: 1/x if x != 0 else np.nan)

# Visualize the data after the transformation
df_inverse[columns_to_normalize].hist(bins=50, figsize=(12, 8), color='purple')
plt.suptitle('After Inverse Normalization')
plt.tight_layout(pad=0.5)
plt.show()

# Log Normalization (Works the Best Out of all the Normalization Techniques Used)

df_log = df.copy()

columns_to_normalize = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6',
                       'X7', 'X8', 'X9', 'X10', 'X11', 'X12',
                       'X13', 'X14', 'X15', 'X16', 'X17', 'X18']

# Visualize the data before applying the transformation
df_log[columns_to_normalize].hist(bins=50, figsize=(12, 8), color='blue')
plt.suptitle('Before Log Normalization')
plt.tight_layout(pad=0.5)
plt.show()

# Apply the log transformation
for column in columns_to_normalize:
    df_log[column] = df_log[column].apply(lambda x: np.log1p(x) if x > 0 else 0)

# Replace any remaining inf values with NaN
df_log.replace([np.inf, -np.inf], np.nan, inplace=True)

# Visualize the data after the transformation
df_log[columns_to_normalize].hist(bins=50, figsize=(12, 8), color='green')
plt.suptitle('After Log Normalization')
plt.tight_layout(pad=0.5)
plt.show()

# df is now the original dataframe with log-transformed values
df = df_log

"""

# Z-Score Normalization

df_z_score = df.copy()

columns_to_normalize = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6',
                       'X7', 'X8', 'X9', 'X10', 'X11', 'X12',
                       'X13', 'X14', 'X15', 'X16', 'X17', 'X18']

# Visualize the data before the transformation
df_z_score[columns_to_normalize].hist(bins=50, figsize=(12, 8))
plt.suptitle('Before Z-Score Normalization')
plt.tight_layout(pad=0.5)
plt.show()

# Apply the z-score transformation
for column in columns_to_normalize:
    mean = df_z_score[column].mean()
    std = df_z_score[column].std()
    df_z_score[column] = (df_z_score[column] - mean) / std

# Visualize the data after the transformation
df_z_score[columns_to_normalize].hist(bins=50, figsize=(12, 8))
plt.suptitle('After Z-Score Normalization')
plt.tight_layout(pad=0.5)
plt.show()

"""

# Outlier Detection

features_to_plot = df.columns[df.columns != 'status_encoding']

for feature in features_to_plot:
    Q1 = df[feature].quantile(0.25)
    Q2 = df[feature].quantile(0.50)
    Q3 = df[feature].quantile(0.75)
    IQR = Q3 - Q1
    lowerBound = Q1 - 1.5 * IQR
    upperBound = Q3 + 1.5 * IQR
    df[feature] = df[feature].clip(lower=lowerBound, upper=upperBound)

num_features = len(features_to_plot)
num_cols = 3
num_rows = math.ceil(num_features / num_cols)

plt.figure(figsize=(12, 2 * num_rows))
for i, feature in enumerate(features_to_plot, 1):
    plt.subplot(num_rows, num_cols, i)
    sns.boxplot(x=df[feature])
    plt.title(f'Box Plot of {feature} After Handling Outliers')
plt.tight_layout()
plt.show()

# Feature Selection Heatmap (The Best Method)

# Calculate the correlation matrix
correlation_matrix = df.corr().abs()

# Create a mask for the lower triangle to avoid the mirror values
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Invert the mask to keep the upper triangle
mask = np.invert(mask)

# Create a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)
plt.title('Feature Correlation Heatmap')
plt.tight_layout(pad=2.0)
plt.show()

# Select upper triangle of the correlation matrix
upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))

# Find the index of feature columns with a correlation greater than the threshold
threshold = 0.85
to_drop = [column for column in upper.columns if any(upper[column] > threshold)]

# Drop those features
df_reduced = df.drop(columns=to_drop)

print(f"Dropped columns: {to_drop}")

# Recalculate the correlation matrix after dropping the features
correlation_matrix_reduced = df_reduced.corr().abs()

# Create a mask for the lower triangle to avoid the mirror values
mask_reduced = np.triu(np.ones_like(correlation_matrix_reduced, dtype=bool))

# Invert the mask to keep the upper triangle
mask_reduced = np.invert(mask_reduced)

# Create a heatmap with the updated correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix_reduced, mask=mask_reduced, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)
plt.title('Feature Correlation Heatmap (After Dropping Highly Correlated Features)')
plt.tight_layout(pad=2.0)
plt.show()

"""

# Feature Selection Variance Inflation Factor (OLD VERSION)

# Function to calculate VIF
def calculate_vif(df):
    vif_data = pd.DataFrame()
    vif_data["feature"] = df.columns

    # Calculate VIF for each feature
    vif_data["VIF"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    return vif_data

# Drop any columns with zero or near-zero variance before calculating VIF
def drop_zero_variance_features(df):
    non_constant_columns = [col for col in df.columns if df[col].std() > 1e-10]  # threshold for near-zero variance
    return df[non_constant_columns]

# Check for near-zero variance
df_non_constant = drop_zero_variance_features(df)

# Calculate VIF
vif_df = calculate_vif(df_non_constant)

# Display VIF DataFrame
print(vif_df)

# Drop features with VIF > 5 (or another threshold)
df_reduced = df.drop(columns=vif_df[vif_df['VIF'] > 5]['feature'])

print(f"Dropped columns: {vif_df[vif_df['VIF'] > 5]['feature'].tolist()}")

"""

"""

# This works almost the best

# Function to calculate VIF
def calculate_vif(df):
    vif_data = pd.DataFrame()
    vif_data["feature"] = df.columns

    # Calculate VIF for each feature
    vif_data["VIF"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    return vif_data

# Drop any columns with zero or near-zero variance before calculating VIF
def drop_zero_variance_features(df):
    non_constant_columns = [col for col in df.columns if df[col].std() > 1e-10]  # threshold for near-zero variance
    return df[non_constant_columns]

# Drop features with perfect multicollinearity
def drop_perfect_multicollinearity(df):
    corr_matrix = df.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    # Find columns with perfect correlation
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] == 1.0)]
    return df.drop(columns=to_drop)

# Check for near-zero variance
df_non_constant = drop_zero_variance_features(df)

# Check for perfect multicollinearity
df_non_collinear = drop_perfect_multicollinearity(df_non_constant)

# Calculate VIF
vif_df = calculate_vif(df_non_collinear)

# Display VIF DataFrame
print("VIF before dropping high VIF features:")
print(vif_df)

# Drop features with VIF > 5 (or another threshold)
threshold = 5
features_to_drop = vif_df[vif_df['VIF'] > threshold]['feature']
df_reduced = df_non_collinear.drop(columns=features_to_drop)

# Display the reduced DataFrame
print(f"Dropped columns: {features_to_drop.tolist()}")
print("DataFrame after dropping high VIF features:")
print(df_reduced.head())

# Check if there are multiple features remaining
if df_reduced.shape[1] > 1:
    # Recalculate VIF after dropping high VIF features
    vif_df_reduced = calculate_vif(df_reduced)
    print("VIF after dropping high VIF features:")
    print(vif_df_reduced)
else:
    print("Not enough features remaining to calculate VIF.")

"""

# Feature Selection by Feature Importance (Random Forest)

# Data Scaling/Standardization with Z-Score Scaling

scaler = StandardScaler()

X = df_reduced.iloc[:, :-1]
y = df_reduced['status_encoding']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=21, test_size=0.20, shuffle=True)

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

train_scaled = pd.concat([pd.DataFrame(X_train_scaled, columns=X.columns), pd.DataFrame(y_train.reset_index(drop=True), columns =['status_encoding'])], axis=1)
test_scaled = pd.concat([pd.DataFrame(X_test_scaled, columns=X.columns), pd.DataFrame(y_test.reset_index(drop=True), columns =['status_encoding'])], axis=1)
df_scaled = pd.concat([train_scaled, test_scaled], axis=0)

print('Dataset After Z-Score Scaling:')
display(df_scaled.head(10))

print('\n')

diagram_columns = 3
attribute_columns = df_scaled.shape[1] - 1
diagram_rows = math.ceil(attribute_columns / diagram_columns)

row_index = 0
column_index = 0
fig, axes = plt.subplots(diagram_rows, diagram_columns, figsize=(18, diagram_rows * 3))
for column in df_scaled.columns[:-1]:
    sns.boxplot(x='status_encoding', y=column, data=df_scaled, ax=axes[row_index, column_index])
    axes[row_index, column_index].set_title(f'Box Plot of {column} After Z-Score Scaling')
    axes[row_index, column_index].set_xlabel('Bankruptcy Status')
    axes[row_index, column_index].set_ylabel(f'{column}')
    column_index += 1
    if column_index == diagram_columns:
        row_index += 1
        column_index = 0
plt.tight_layout()
plt.show()

print('\n')

row_index = 0
column_index = 0
fig, axes = plt.subplots(diagram_rows, diagram_columns, figsize=(18, diagram_rows * 3))
for column in df_scaled.columns[:-1]:
    sns.violinplot(x='status_encoding', y=column, data=df_scaled, ax=axes[row_index, column_index])
    axes[row_index, column_index].set_title(f'Violin Plot of {column} After Z-Score Scaling')
    axes[row_index, column_index].set_xlabel('Bankruptcy Status')
    axes[row_index, column_index].set_ylabel(f'{column}')
    column_index += 1
    if column_index == diagram_columns:
        row_index += 1
        column_index = 0
plt.tight_layout()
plt.show()

# Applying SMOTE for Oversampling the Minority Class

# Applying SMOTE to balance the training data
smote = SMOTE(random_state=21)

# Balancing the training data
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("Before Oversampling the Majority Class:")
print(pd.Series(y_train).value_counts())

print('\n')

print("After Oversampling the Majority Class:")
print(pd.Series(y_train_smote).value_counts())

X_train_balanced_smote = X_train_smote.to_numpy()
y_train_balanced_smote = y_train_smote.to_numpy()

# Employing Hybrid Sampling with SMOTEENN

smoteenn = SMOTEENN(random_state=21)
X_train_smoteenn, y_train_smoteenn = smoteenn.fit_resample(X_train, y_train)

print("Before Hybrid-sampling the Majority Class:")
print(pd.Series(y_train).value_counts())

print('\n')

print("After Hybrid-Sampling the Majority Class:")
print(pd.Series(y_train_smoteenn).value_counts())

X_train_balanced_smoteenn = X_train_smoteenn.to_numpy()
y_train_balanced_smoteenn = y_train_smoteenn.to_numpy()

# Undersampling the Majority Class (Works the Best for Logistic Regression and Random Forest)

df_temp = train_scaled.copy()

df_class_0 = df_temp[df_temp['status_encoding'] == 0]
df_class_1 = df_temp[df_temp['status_encoding'] == 1]

df_class_0_undersampled = resample(df_class_0, replace=False, n_samples=len(df_class_1), random_state=21)

df_balanced = pd.concat([df_class_0_undersampled, df_class_1], axis=0)

df_balanced = df_balanced.sample(frac=1, random_state=21).reset_index(drop=True)

print("Before Undersampling the Majority class:")
print(pd.Series(y_train).value_counts())

print('\n')

print("After Undersampling the Majority Class:")
print(df_balanced['status_encoding'].value_counts())

X_train_balanced_under = df_balanced.drop(columns=['status_encoding']).to_numpy()
y_train_balanced_under = df_balanced['status_encoding'].to_numpy()

# Applying the Random Forest Model

"""

X = df_reduced.drop(columns=['status_encoding'])
y = df_reduced['status_encoding']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""

random_forest = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=None)

# Train the classifier
random_forest.fit(X_train_balanced_under, y_train_balanced_under)

y_pred = random_forest.predict(X_test_scaled)

print("Random Forest Classifier Results:\n")

print("Accuracy:", accuracy_score(y_test, y_pred))

print('\n')

print("Classification Report:")
print(classification_report(y_test, y_pred))

# Visualization
importances = random_forest.feature_importances_
indices = np.argsort(importances)[::-1]
feature_names = X.columns

plt.figure(figsize=(12, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), feature_names[indices], rotation=90)
plt.tight_layout()
plt.show()

# Applying the Logistic Regression Model

param_grid = {
    'penalty': ['l1', 'l2'], # L1 and L2 regularization
    'C': [0.01, 0.1, 1, 10, 100], # C is 1 / lambda (i.e. the inverse of the regularization parameter)
    'max_iter': [100, 200, 300, 400] # The maximum number of iterations, which is similar, but not analogous to epochs
}

model = LogisticRegression(solver='liblinear')

grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)

grid_result = grid.fit(X_train_balanced_under, y_train_balanced_under)

best_params = grid_result.best_params_
best_score = grid_result.best_score_

print('The Best Model\'s Hyper-Parameters:', best_params)
print('Accuracy of the Best Model: {:.2f}%'.format(best_score * 100))

best_model = LogisticRegression(penalty=best_params['penalty'], C=best_params['C'], solver='liblinear', max_iter=best_params['max_iter'])

history = best_model.fit(X_train_balanced, y_train_balanced) # Can use history for plotting useful graphs

y_pred = best_model.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)

print('Accuracy of the Best Model on the Testing Data with Default Threshold: {:.2f}%'.format(accuracy * 100))

y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]

threshold = 0.5

y_pred_adjusted = (y_pred_proba >= threshold).astype(int)

print(classification_report(y_test, y_pred_adjusted))

"""

accuracy = accuracy_score(y_test, y_pred_adjusted)
print('Accuracy of the Best Model on the Testing Data with Threshold {:.2f}: {:.2f}%'.format(threshold, accuracy * 100))

precision = precision_score(y_test, y_pred_adjusted)
print('Precision of the Best Model on the Testing Data with Threshold {:.2f}: {:.2f}%'.format(threshold, precision * 100))

recall = recall_score(y_test, y_pred_adjusted)
print('Recall of the Best Model on the Testing Data with Threshold {:.2f}: {:.2f}%'.format(threshold, recall * 100))

f1 = f1_score(y_test, y_pred_adjusted)
print('F1-Score of the Best Model on the Testing Data with Threshold {:.2f}: {:.2f}%'.format(threshold, f1 * 100))

print(f'Accuracy of the Best Model With the Threshold {threshold}: {accuracy * 100:.2f}%')

"""

# Plot Results

print('Logistic Regression Result Plots\n')

cm = confusion_matrix(y_test, y_pred_adjusted)

plt.figure(figsize=(10, 4))

# Visualize correlation matrix using heatmap
plt.subplot(1, 2, 1)
sns.heatmap(cm, annot=True, cmap='coolwarm', fmt='d')
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
# plt.show()

# Generate model predictions and probabilities
y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]  # Probabilities for the positive class

# Compute FPR, TPR, and thresholds for ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Calculate the AUC
auc = roc_auc_score(y_test, y_pred_proba)

# Plot the ROC curve
plt.subplot(1, 2, 2)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})', color='blue', linewidth=2)
plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier', linewidth=1)  # Diagonal for random classifier
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.legend(loc='lower right')
plt.grid()

plt.tight_layout()
plt.show()

# XG Boost code.

param_grid = { # subsample, colsample, n_estimators, max_depth, and gamma are all special fields to tune for the trees in XG boost.
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2], # common to all models
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2]
}

# Initialize the classifier with logloss
model = XGBClassifier(eval_metric='logloss')



# Set up the GridSearchCV object
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)

# Fit the grid search to the training data
grid_result = grid.fit(X_train_scaled, y_train)

# Extract the best hyperparameters and accuracy
best_params = grid_result.best_params_
best_score = grid_result.best_score_

# Print the results
print('The Best Model\'s Hyper-Parameters:', best_params)
print('Accuracy of the Best Model: {:.2f}%'.format(best_score * 100))

# Part 2: Testing

best_model = XGBClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], eval_metric='logloss', learning_rate=best_params['learning_rate'], subsample=best_params['subsample'], colsample_bytree=best_params['colsample_bytree'], gamma=best_params['gamma'])

history = best_model.fit(X_train_scaled, y_train) # Can use history for plotting useful graphs

y_pred = best_model.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)

print('Accuracy of the Best Model on the Testing Data: {:.2f}%'.format(accuracy * 100))

# Part 3: Graph a heat map for analyzing the XG Boost model.

y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]

threshold = 0.5

y_pred_adjusted = (y_pred_proba >= threshold).astype(int)

accuracy = accuracy_score(y_test, y_pred_adjusted)

print(f'Accuracy of the Best Model With the Threshold {threshold}: {accuracy * 100:.2f}%')

cm = confusion_matrix(y_test, y_pred_adjusted)

# Visualize correlation matrix using heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, cmap='coolwarm', fmt='d')
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.show()

joblib.dump(X_train.mean(), 'col_averages.pkl') # Save the column averages for data imputation
joblib.dump(scaler, 'scaler.pkl') # Save the scaler
joblib.dump(model, 'best_model.pkl') # Save the best model